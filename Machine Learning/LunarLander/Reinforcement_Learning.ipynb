{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:672: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\")\n",
    "env.reset()\n",
    "\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action = 1\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilość możliwych akcji: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'Ilość możliwych akcji: {env.action_space.n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: CPU\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZE_WITH_HARDWARE = False\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if OPTIMIZE_WITH_HARDWARE:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(f'Selected device: MPS (Metal Performance Shaders)')\n",
    "    elif torch.backends.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f'Selected device: GPU with CUDA support')\n",
    "else:\n",
    "    print(f'Selected device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.nn.functional.leaky_relu(self.fc1(state), negative_slope=0.01)\n",
    "        x = torch.nn.functional.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametry treningu sieci DQN\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.state_size = state_size        # ilość informacji dot. stanu środowiska\n",
    "        self.action_size = action_size      # ilość akcji, które agent może wykonać\n",
    "        self.discount_factor = 0.99         # współczynnik spadku wartości nagrody\n",
    "        self.epsilon_greedy = 1.0           # początkowy współczynnik losowości (1 = 100% losowości)\n",
    "        self.epsilon_greedy_min = 0.1       # minimalny współczynnik losowości\n",
    "        self.epsilon_greedy_decay = 0.995   # zmniejszanie stopnia losowości co iterację o 5%\n",
    "        self.memory = deque(maxlen=1000)    # kolekcja przechowująca 1000 ostatnich zdarzeń\n",
    "        self.train_start = 500              # liczba zdarzeń, od której zaczynamy trenować model\n",
    "\n",
    "        self.model = DQNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    # Zapisuje podjętą akcję w danym stanie i jej skutki \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    # Wybiera akcje dla danego stanu. Jeśli aktualnie model\n",
    "    # nie eksploruje (wykonuje losową akcje) to wybierana jest\n",
    "    # akcja o najlepszym potencjale (najwyższa wartość nagrody)\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon_greedy:\n",
    "            return random.randrange(self.action_size)\n",
    "        # unsqueeze zapewnia odpowiedni wymiar [batch_size, state_size]\n",
    "        # PyTorch narzuca format danych treningowych w postaci tensora, który\n",
    "        # w pierwszym wymiarze zawiera informację i ilości paczek a następnie same\n",
    "        # dane treningowe, dlatego 'unsqueeze' rozszerza wymiar danych mimo tego, że\n",
    "        # mamy tylko jedną paczkę w tej funkcji\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values_predicted = self.model(state)\n",
    "        return torch.argmax(q_values_predicted).item()\n",
    "    \n",
    "\n",
    "    def replay(self):\n",
    "        # Nie zaczynamy trenować modelu dopóki nie zbierzemy\n",
    "        # minimalnej ilości danych w buforze memory\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        data_batch = random.sample(self.memory, BATCH_SIZE) # Losujemy paczkę danych do treningu\n",
    "        \n",
    "        total_mse_loss = 0\n",
    "        for state, action, reward, next_state, done in data_batch:\n",
    "            state = torch.FloatTensor(state)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            reward = torch.FloatTensor([reward])\n",
    "            discounted_reward = reward\n",
    "            if not done:\n",
    "                discounted_reward += self.discount_factor * torch.max(self.model(next_state))\n",
    "            \n",
    "            dqn_prediction = self.model(state)\n",
    "            true_reward = dqn_prediction.clone()     # Tworzymy klon aby nadpisać wynik dla akcji niżej\n",
    "            true_reward[action] = discounted_reward  # Nadpisujemy wartość nagrody dla wykonanej akcji\n",
    "            \n",
    "            loss = self.criterion(dqn_prediction, true_reward)\n",
    "            \n",
    "            self.optimizer.zero_grad()  # Zerujemy gradient\n",
    "            loss.backward()             # Liczymy gradient\n",
    "            self.optimizer.step()       # Aktualizujemy wagi sieci\n",
    "\n",
    "            total_mse_loss += loss.item()\n",
    "        \n",
    "        # Jeśli nie doszliśmy do minimalnej wartości współczynnika\n",
    "        # eksploracji to nadal go zmniejszamy z każdą iteracją\n",
    "        if self.epsilon_greedy > self.epsilon_greedy_min:\n",
    "            self.epsilon_greedy *= self.epsilon_greedy_decay\n",
    "        \n",
    "        return total_mse_loss / BATCH_SIZE # zwracamy średni błąd MSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
